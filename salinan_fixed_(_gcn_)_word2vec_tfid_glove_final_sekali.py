# -*- coding: utf-8 -*-
"""Salinan Fixed ( GCN ) - Word2Vec-TFID-GLOVE FINAL SEKALI.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SW_TVQ-pFsN27Lapf6XoypWH20Rw0tFA
"""

wget http://doug.my.id/storage/20191002-reviews.xlsx
wget http://doug.my.id/storage/human_review.xlsx

!pip install scikit-learn
!pip install Sastrawi
!pip install torch_geometric
!pip install sacrebleu
!pip install anthropic
!pip install wordcloud

devMode = "false"

from Sastrawi.Stemmer.StemmerFactory import StemmerFactory
import pandas as pd
from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory
import re
import numpy as np
from nltk.tokenize import word_tokenize
import nltk
import string
import torch
import torch.nn as nn
from torch.nn import Parameter
from torch import optim
import torch.nn.functional as F
from torch_geometric.nn import GCNConv
import networkx as nx
from collections import defaultdict

BASE_PATH = "./"
DATASET_PATH = "20191002-reviews.xlsx"
HR_PATH = "human_review.xlsx"

review_df = pd.read_excel(BASE_PATH + DATASET_PATH)
human_review_df = pd.read_excel(BASE_PATH + HR_PATH)

# Preprocessing

review_df = review_df.loc[:, ["itemId", "name", "reviewTitle", "reviewContent"]]

# Remove NaN dan null
review_df = review_df.replace(['nan', 'null'], "").fillna("")

#all symbol remove
#example case ðÿx08fðÿx08fðÿx08f
review_df = review_df.replace(r'[^\x00-\x7F]+', '', regex=True)

# Remove duplicate data
review_df = review_df.drop_duplicates()

# Create column review = reviewTitle + reviewContent
review_df["reviewTitle"] = review_df["reviewTitle"].astype(str)
review_df["reviewContent"] = review_df["reviewContent"].astype(str)
review_df["review"] = review_df.apply(lambda row: row['reviewTitle'] + ' ' + row['reviewContent'], axis=1)

# Remove duplicate characters
factory = StemmerFactory()
stemmer = factory.create_stemmer()

review_df.head()

human_review_df.head()

import nltk
nltk.download('punkt_tab')

from sklearn.feature_extraction.text import TfidfVectorizer # Import TfidfVectorizer

def remove_duplicate_chars(text):
    special_words = ['good']
    for i, word in enumerate(special_words):
        text = text.replace(word, f"_{i}_")
    no_double_text = re.sub(r'(.)\1+', r'\1', text)
    for i, word in enumerate(special_words):
        no_double_text = no_double_text.replace(f"_{i}_", word)
    return no_double_text

review_df["review"] = review_df.apply(lambda row: remove_duplicate_chars(row["review"]), axis=1)

# Stopword Removal
def stopword_removal(text):
    factory = StopWordRemoverFactory()
    stopword = factory.create_stop_word_remover()
    text_clean = stopword.remove(text)
    return text_clean

review_df["review"] = review_df.apply(lambda row: stopword_removal(row["review"]), axis=1)

# Remove Punctuation
def remove_punctuation(text):
    translator = str.maketrans('', '', string.punctuation)
    return text.translate(translator)

review_df['review'] = review_df['review'].apply(remove_punctuation)

# Group By Item ID
review_df = review_df.groupby(['itemId'])["review"].agg(lambda x: ' '.join(x)).reset_index()

review_df = review_df.merge(human_review_df, on='itemId')

# Tokenization
def tokenized_text(text):
    tokenized_text = word_tokenize(text)
    #remove if word < 1 character
    tokenized_text = [word for word in tokenized_text if len(word) > 1]
    return tokenized_text

review_df["tokenized"] = review_df.apply(lambda row: tokenized_text(row["review"]), axis=1)

tfidf = TfidfVectorizer()
processed_docs = [re.sub(r'\s+', ' ', doc.lower()) for doc in review_df.loc[review_df['itemId'] == 6070, 'tokenized'].values[0]]

# Fit and transform the 'review' column
tfidf_matrix = tfidf.fit_transform(processed_docs)

review_df.to_csv('dataset_merged_df.csv', index=False)

review_df.head()

"""### Filter dataset supaya data yang kita proses sesuai dg yg direview expert judgment"""

# Make sure 'Review Expert.csv' is in the correct path,
# then replace 'Review Expert.csv' with the actual file path if necessary.
# If the file is in a different directory, you need to specify the
# full path, for example:
# df_expert = pd.read_csv('/path/to/your/file/Review Expert.csv', sep=";", usecols=["review", "Summary"]).rename(columns={"Summary": "summary_expert"}).head(125)
df_expert = pd.read_csv('Review Expert.csv', sep=";", usecols=["review", "Summary"]).rename(columns={"Summary": "summary_expert"}).head(125)
df_expert.tail()

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity


# Function to preprocess text (you can adjust this based on your needs)
def preprocess_text(text):
    text = text.lower()
    # Add other preprocessing steps as needed
    return text

# Preprocess the 'review' columns
review_df['processed_review'] = review_df['review'].apply(preprocess_text)
df_expert['processed_review'] = df_expert['review'].apply(preprocess_text)


# Create TF-IDF vectors
vectorizer = TfidfVectorizer()
tfidf_matrix_reviews = vectorizer.fit_transform(review_df['processed_review'])
tfidf_matrix_expert = vectorizer.transform(df_expert['processed_review'])

# Calculate cosine similarity
cosine_similarities = cosine_similarity(tfidf_matrix_expert, tfidf_matrix_reviews)

# Find the most similar review for each expert review
matched_indices = []
for i in range(len(df_expert)):
    most_similar_index = cosine_similarities[i].argmax()
    matched_indices.append(most_similar_index)


# Create the new DataFrame
merged_df = pd.DataFrame()

for i in range(len(df_expert)):
    expert_row = df_expert.iloc[i]
    review_row = review_df.iloc[matched_indices[i]]

    # Combine the rows into a single row
    combined_row = pd.concat([expert_row, review_row])

    # Append the combined row to the merged_df
    merged_df = pd.concat([merged_df, combined_row.to_frame().T], ignore_index=True)


# Display the merged DataFrame (optional)
merged_df = merged_df[['itemId','review',
       'human_review', 'tokenized', 'summary_expert',]]

merged_df

review_df = merged_df

review_df

# !pip install transformers

# prompt: please training dataset review_df["review"] with model BERT
# from transformers import BertTokenizer, BertModel
# import pandas as pd
# import torch

# # Load pre-trained model tokenizer (vocabulary)
# tokenizer = BertTokenizer.from_pretrained('indobenchmark/indobert-base-p2')
# model = BertModel.from_pretrained('indobenchmark/indobert-base-p2')

# review_df = pd.read_csv('dataset_merged_df.csv')

# # Function to generate BERT embeddings
# def get_bert_embeddings(text):
#     if not isinstance(text, str):  # Check if the input is a string
#         text = str(text) if pd.notnull(text) else ""
#     inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=512)
#     with torch.no_grad():
#         outputs = model(**inputs)
#     # Use the [CLS] token embeddings as sentence embeddings
#     embeddings = outputs.last_hidden_state[:, 0, :].numpy()
#     return embeddings

# # Apply the function to the "review" column
# review_df['bert_embeddings'] = review_df['review'].apply(get_bert_embeddings)

# # Save embeddings to a new CSV file
# review_df.to_csv('dataset_merged_df_with_bert.csv', index=False)
# review_df.head()

# prompt: please training dataset review_df["review"] with model TF-IDF (TfidfVectorizer)

from sklearn.feature_extraction.text import TfidfVectorizer

tfidf = TfidfVectorizer()
#tfidf = TfidfVectorizer(stop_words=None,  # or provide a custom stop word list
#                        token_pattern=r"(?u)\b\w+\b",  # Match single characters as well
#                        min_df=1)  # Consider words that appear in at least 1 document

processed_docs = [re.sub(r'\s+', ' ', doc.lower()) for doc in review_df.loc[review_df['itemId'] == 6070, 'tokenized'].values[0]]
print("processed_docs:\n", processed_docs)
# Fit and transform the 'review' column
tfidf_matrix = tfidf.fit_transform(processed_docs)

# Get feature names (words)
feature_names = tfidf.get_feature_names_out()

# Convert the sparse matrix to a dense matrix (if needed for easier analysis)
dense_matrix = tfidf_matrix.toarray()

# You can create a DataFrame from the dense matrix to visualize the TF-IDF scores
tfidf_df = pd.DataFrame(dense_matrix, columns=feature_names)

# Print or analyze the TF-IDF matrix or DataFrame
#tfidf_df
#processed_docs

# You can now use this TF-IDF matrix as input for your machine learning model.

# prompt: plot the graph

import matplotlib.pyplot as plt

# Assuming you have a DataFrame named 'tfidf_df'
# You can create a bar plot of the TF-IDF scores for a specific document (e.g., document 0)

document_index = 0
tfidf_scores = tfidf_df.iloc[document_index]

# Sort the TF-IDF scores in descending order
sorted_tfidf_scores = tfidf_scores.sort_values(ascending=False)

# Get the top N words with highest TF-IDF scores
top_n = 10
top_tfidf_scores = sorted_tfidf_scores[:top_n]

# Create the bar plot
plt.figure(figsize=(10, 6))
plt.bar(top_tfidf_scores.index, top_tfidf_scores.values)
plt.xlabel("Words")
plt.ylabel("TF-IDF Score")
plt.title(f"Top {top_n} Words with Highest TF-IDF Scores for Document {document_index}")
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

# prompt: create sample TF-IDF
def GraphTFIDF(text):
    # Create a TfidfVectorizer object
    tfidf = TfidfVectorizer()
    # Fit and transform the 'review' column
    tfidf_matrix = tfidf.fit_transform(text)
    # Get feature names (words)
    feature_names = tfidf.get_feature_names_out()
    df_tfidf = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names)

    words_freq = defaultdict(int)
    for word in text:
        words_freq[word] += 1
    n_words = len(words_freq)
    word_idx = dict(zip(words_freq.keys(), range(n_words)))
    # adjacency matrix with TF-IDF
    adj_matrix = np.zeros((n_words, n_words))
    for word_i in word_idx:
        for word_j in word_idx:
            if word_i != word_j:
                adj_matrix[word_idx[word_i], word_idx[word_j]] = df_tfidf[word_i.lower()].corr(df_tfidf[word_j.lower()])

    # create graph from adjacency matrix
    graph = nx.Graph(adj_matrix)
    labels = {idx: word for word, idx in word_idx.items()}
    graph = nx.relabel_nodes(graph, labels)

    features = np.array([tfidf_matrix[i].toarray()[0] for i in range(tfidf_matrix.shape[0])])

    return adj_matrix, features, graph
    # return 'a', 'f', 'g'

# Example usage
# selected_text = review_df.loc[review_df['itemId'] ==  49780, 'tokenized'].values[0]

# Check if there is a row with itemId == 49780
if review_df[review_df['itemId'] == 49780].empty:
    print("No row found with itemId == 49780. Using a different itemId.")
    # Choose a different itemId that exists in the DataFrame
    selected_itemId = review_df['itemId'].iloc[0]  # Using the first itemId as an example
    selected_text = review_df.loc[review_df['itemId'] == selected_itemId, 'tokenized'].values[0]
else:
    selected_text = review_df.loc[review_df['itemId'] == 49780, 'tokenized'].values[0]

a, f, g = GraphTFIDF(selected_text)

#function Adjust features to match the number of nodes in adjacency matrix

def AdjustFeatures(f, a):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    a = a.astype(np.float32)
    f = torch.FloatTensor(f).to(device)

    if f.shape[0] > a.shape[0]:
      f = f[:a.shape[0], :]
    elif f.shape[0] < a.shape[0]:
      a = a[:f.shape[0], :f.shape[0]]

    return f, a, device

# torch.device("cuda" if torch.cuda.is_available() else "cpu")

f, a , device= AdjustFeatures(f, a)
# Check shapes after adjustment
print("Adjusted Shape of adjacency:", a.shape)
print("Adjusted Shape of features:", f.shape)

nx.draw(g, with_labels=True, font_weight='bold', font_color='brown')
plt.show()

!pip install gensim

# Create Graph with Word2Vec

from gensim.models import Word2Vec

def GraphWord2Vec(text):
    # Train Word2Vec model
    w2v_model = Word2Vec([text], vector_size=64, window=2, min_count=1, sg=0)
    word_vectors = w2v_model.wv

    words_freq = defaultdict(int)
    for word in text:
        words_freq[word] += 1
    n_words = len(words_freq)
    word_idx = dict(zip(words_freq.keys(), range(n_words)))

    # adjacency matrix with Word2Vec similarity
    adj_matrix = np.zeros((n_words, n_words))
    for word_i in word_idx:
        for word_j in word_idx:
            if word_i != word_j:
                adj_matrix[word_idx[word_i], word_idx[word_j]] = word_vectors.similarity(word_i, word_j)
    # create graph from adjacency matrix
    graph = nx.Graph(adj_matrix)
    labels = {v: k for k, v in word_idx.items()}
    graph = nx.relabel_nodes(graph, labels)

    features = np.array([word_vectors[word] for word in text])

    return adj_matrix, features, graph

# Example usage
selected_itemId = review_df['itemId'].iloc[0]  # Get the first itemId in the DataFrame
selected_text = review_df.loc[review_df['itemId'] == selected_itemId, 'tokenized'].values[0]
a, f, g = GraphWord2Vec(selected_text)

f, a , device= AdjustFeatures(f, a)
# Check shapes after adjustment
print("Adjusted Shape of adjacency:", a.shape)
print("Adjusted Shape of features:", f.shape)

nx.draw(g, with_labels=True, font_weight='bold', font_color='brown')
plt.show()

